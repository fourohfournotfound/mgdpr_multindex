{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***Pkg Installation & Drive Mount***"
      ],
      "metadata": {
        "id": "q7DOR-Aq6EZ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7dbc-lj5TRn"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "import os\n",
        "\n",
        "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "sys.path.append('path_to_your_code')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Datasets Generation***"
      ],
      "metadata": {
        "id": "csT3-2_k6N9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.linalg import expm\n",
        "import torch\n",
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import sklearn.preprocessing as skp\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple\n",
        "from functools import lru_cache\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, root: str, desti: str, market: str, comlist: List[str], start: str, end: str, window: int, dataset_type: str):\n",
        "        super().__init()\n",
        "\n",
        "        self.comlist = comlist\n",
        "        self.market = market\n",
        "        self.root = root\n",
        "        self.desti = desti\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.window = window\n",
        "        self.dates, self.next_day = self.find_dates(self.start, self.end, self.root, self.comlist, self.market)\n",
        "        self.dataset_type = dataset_type\n",
        "\n",
        "        # Check if graph files already exist\n",
        "        graph_files_exist = all(os.path.exists(os.path.join(self.desti, f'{self.market}_{self.dataset_type}_{self.start}_{self.end}_{self.window}/graph_{i}.pt')) for i in range(len(self.dates) - self.window + 1))\n",
        "\n",
        "        if not graph_files_exist:\n",
        "            # Generate the graphs and save them as PyTorch tensors\n",
        "            self._create_graphs(self.dates, self.desti, self.comlist, self.market, self.root, self.window, self.next_day)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dates) - self.window + 1\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        directory_path = os.path.join(self.desti, f'{self.market}_{self.dataset_type}_{self.start}_{self.end}_{self.window}')\n",
        "        data_path = os.path.join(directory_path, f'graph_{idx}.pt')\n",
        "\n",
        "        if os.path.exists(data_path):\n",
        "            return torch.load(data_path)\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"No graph data found for index {idx}. Please ensure you've generated the required data.\")\n",
        "\n",
        "    def check_years(self, date_str: str, start_str: str, end_str: str) -> bool:\n",
        "        date_format = \"%Y-%m-%d\"\n",
        "        date = datetime.strptime(date_str, date_format)\n",
        "        start = datetime.strptime(start_str, date_format)\n",
        "        end = datetime.strptime(end_str, date_format)\n",
        "\n",
        "        return start <= date <= end\n",
        "\n",
        "    def find_dates(self, start: str, end: str, path: str, comlist: List[str], market: str) -> Tuple[List[str], str]:\n",
        "        # Get the dates for each company in the target list\n",
        "        date_sets = []\n",
        "        after_end_date_sets = []\n",
        "        for h in comlist:\n",
        "            dates = set()\n",
        "            after_end_dates = set()\n",
        "            d_path = os.path.join(path, f'{market}_{h}_30Y.csv')\n",
        "            with open(d_path, 'r') as f:\n",
        "                file = csv.reader(f)\n",
        "                next(file, None)  # Skip the header row\n",
        "                for line in file:\n",
        "                    date_str = line[0][:10]\n",
        "                    if self.check_years(date_str, start, end):\n",
        "                        dates.add(date_str)\n",
        "                    elif self.check_years(date_str, end, '2017-12-31'): # '2017-12-31' is just an example, if the latest data is used, fill in the current date\n",
        "                        after_end_dates.add(date_str)\n",
        "\n",
        "            date_sets.append(dates)\n",
        "            after_end_date_sets.append(after_end_dates)\n",
        "\n",
        "        # Find the intersection of all date sets\n",
        "        all_dates = list(set.intersection(*date_sets))\n",
        "        all_after_end_dates = list(set.intersection(*after_end_date_sets))\n",
        "\n",
        "        # Find the next common day after the end date\n",
        "        next_common_day = min(all_after_end_dates) if all_after_end_dates else None\n",
        "\n",
        "        return sorted(all_dates), next_common_day\n",
        "\n",
        "    def signal_energy(self, x_tuple: Tuple[float]) -> float:\n",
        "        x = np.array(x_tuple)\n",
        "        return np.sum(np.square(x))\n",
        "\n",
        "    def information_entropy(self, x_tuple: Tuple[float]) -> float:\n",
        "        x = np.array(x_tuple)\n",
        "        unique, counts = np.unique(x, return_counts=True)\n",
        "        probabilities = counts / np.sum(counts)\n",
        "        entropy = -np.sum(probabilities * np.log(probabilities))\n",
        "\n",
        "        return entropy\n",
        "\n",
        "    def adjacency_matrix(self, X: torch.Tensor) -> torch.Tensor:\n",
        "        A = torch.zeros((X.shape[0], X.shape[0]))\n",
        "        X = X.numpy()\n",
        "        energy = np.array([self.signal_energy(tuple(x)) for x in X])\n",
        "        entropy = np.array([self.information_entropy(tuple(x)) for x in X])\n",
        "        for i in range(X.shape[0]):\n",
        "            for j in range(X.shape[0]):\n",
        "                A[i, j] = torch.tensor((energy[i] / energy[j]) * (math.exp(entropy[i] - entropy[j])), dtype=torch.float32)\n",
        "\n",
        "        return torch.log(A[A<1] = 1)\n",
        "\n",
        "    def node_feature_matrix(self, dates: List[str], comlist: List[str], market: str, path: str) -> torch.Tensor:\n",
        "        # Convert dates to datetime format for easier comparison\n",
        "        dates_dt = [pd.to_datetime(date).date() for date in dates]\n",
        "\n",
        "        # Initialize the tensor\n",
        "        X = torch.zeros((5, len(comlist), len(dates_dt)))\n",
        "\n",
        "        for idx, h in enumerate(comlist):\n",
        "            d_path = os.path.join(path, f'{market}_{h}_30Y.csv')\n",
        "\n",
        "            # Read the entire CSV file into a DataFrame, but only the rows with date in 'dates_dt'\n",
        "            df = pd.read_csv(d_path, parse_dates=[0], index_col=0)\n",
        "            df.index = df.index.astype(str).str.split(\" \").str[0]\n",
        "            df.index = pd.to_datetime(df.index)\n",
        "            df = df[df.index.isin(dates_dt)]\n",
        "\n",
        "            # Transpose the DataFrame so that the dates become columns and the fields become rows\n",
        "            df_T = df.transpose()\n",
        "\n",
        "            # Select only the rows for the fields we're interested in, which are the ones with indices 1 to 5\n",
        "            df_selected = df_T.iloc[0:5]\n",
        "\n",
        "            # Convert the selected part of the DataFrame to a numpy array and assign it to the tensor\n",
        "            X[:, idx, :] = torch.from_numpy(df_selected.to_numpy())\n",
        "\n",
        "        return X\n",
        "\n",
        "    def _create_graphs(self, dates: List[str], desti: str, comlist: List[str], market: str, root: str, window: int, next_day: str):\n",
        "        dates.append(next_day)\n",
        "\n",
        "        # Wrap the range function with tqdm to create a progress bar\n",
        "        for i in tqdm(range(len(dates) - window)):\n",
        "            # Construct the filename for the current graph\n",
        "            directory_path = os.path.join(desti, f'{market}_{self.dataset_type}_{self.start}_{self.end}_{window}')\n",
        "            filename = os.path.join(directory_path, f'graph_{i}.pt')\n",
        "\n",
        "            # If the file already exists, skip to the next iteration\n",
        "            if os.path.exists(filename):\n",
        "                print(f\"Graph {i + 1}/{len(dates) - window} already exists, skipping...\")\n",
        "                continue\n",
        "\n",
        "            print(f'Generating graph {i + 1}/{len(dates) - window}...')\n",
        "\n",
        "            # Generate labels\n",
        "            box = dates[i:i + window + 1]\n",
        "            X = self.node_feature_matrix(box, comlist, market, root)\n",
        "            C = torch.zeros(X.shape[1])\n",
        "            for j in range(C.shape[0]):\n",
        "                if X[3, j, -1] - X[3, j, -2] > 0:\n",
        "                    C[j] = 1\n",
        "\n",
        "            # Slice the desired data and do normalization on raw data\n",
        "            X = X[:, :, :-1]\n",
        "            for i in range(X.shape[0]):\n",
        "                # Adding 1 before taking log to avoid log(0)\n",
        "                X[i] = torch.Tensor(np.log1p(X[i].numpy()))\n",
        "\n",
        "            # Obtain adjacency tensor\n",
        "            A = torch.zeros((X.shape[0], X.shape[1], X.shape[1]))\n",
        "            for j in range(A.shape[0]):\n",
        "                A[j] = self.adjacency_matrix(X[j])\n",
        "\n",
        "            # Save the X, A, C tensors\n",
        "            os.makedirs(directory_path, exist_ok=True)\n",
        "            torch.save({'X': X, 'A': A, 'Y': C}, filename)"
      ],
      "metadata": {
        "id": "7y0nw5Ug6T9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Model Configuration***"
      ],
      "metadata": {
        "id": "LvYtJi386rje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class MultiReDiffusion(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, num_relation):\n",
        "        super(MultiReDiffusion, self).__init__()\n",
        "        self.output = output_dim\n",
        "        self.fc_layers = nn.ModuleList([nn.Linear(input_dim, output_dim) for _ in range(num_relation)])\n",
        "        self.update_layer = torch.nn.Conv2d(num_relation, num_relation, kernel_size=1)\n",
        "        self.activation1 = torch.nn.PReLU()\n",
        "        self.activation0 = torch.nn.PReLU()\n",
        "        self.num_relation = num_relation\n",
        "\n",
        "    def forward(self, theta, t, a, x):\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        diffusions = torch.zeros(theta.shape[0], a.shape[1], self.output).to(device)\n",
        "\n",
        "        for rel in range(theta.shape[0]):\n",
        "            diffusion_mat = torch.zeros_like(a[rel])\n",
        "            for step in range(theta.shape[-1]):\n",
        "                diffusion_mat += theta[rel][step] * t[rel][step] * a[rel]\n",
        "\n",
        "            diffusion_feat = torch.matmul(diffusion_mat, x[rel])\n",
        "            diffusions[rel] = self.activation0(self.fc_layers[rel](diffusion_feat))\n",
        "\n",
        "        latent_feat = self.activation1(self.update_layer(diffusions.unsqueeze(0)))\n",
        "        latent_feat = latent_feat.reshape(self.num_relation, a.shape[1], -1)\n",
        "\n",
        "        return latent_feat, diffusions\n",
        "\n",
        "\n",
        "class ParallelRetention(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, time_dim, in_dim, inter_dim, out_dim):\n",
        "        super(ParallelRetention, self).__init__()\n",
        "        self.time_dim = time_dim\n",
        "        self.in_dim = in_dim\n",
        "        self.inter_dim = inter_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.activation = torch.nn.PReLU()\n",
        "        self.Q_layers = nn.Linear(self.in_dim, self.inter_dim)\n",
        "        self.K_layers = nn.Linear(self.in_dim, self.inter_dim)\n",
        "        self.V_layers = nn.Linear(self.in_dim, self.inter_dim)\n",
        "        self.ret_feat = torch.nn.Linear(self.inter_dim, self.out_dim)\n",
        "\n",
        "    def forward(self, x, d_gamma):\n",
        "        num_node = x.shape[1]\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        d_gamma = d_gamma.to(device)\n",
        "        x = x.view(self.time_dim, -1)\n",
        "\n",
        "        inter_feat = self.Q_layers(x) @ self.K_layers(x).transpose(0, 1)\n",
        "        x = (d_gamma * inter_feat) @ self.V_layers(x)\n",
        "        x = self.activation(self.ret_feat(x))\n",
        "\n",
        "        return x.view(num_node, -1)\n",
        "\n",
        "\n",
        "class MGDPR(nn.Module):\n",
        "    def __init__(self, diffusion, retention, ret_linear_1, ret_linear_2, post_pro,\n",
        "                 layers, num_nodes, time_dim, num_relation, gamma, expansion_steps):\n",
        "        super(MGDPR, self).__init__()\n",
        "\n",
        "        self.layers = layers\n",
        "\n",
        "        # Learnable parameters for multi-relational transitions:\n",
        "        # T is used as a transition (or weighting) tensor.\n",
        "        # Initialized using standard Xavier uniform initialization.\n",
        "        self.T = nn.Parameter(torch.empty(layers, num_relation, expansion_steps, num_nodes, num_nodes))\n",
        "        nn.init.xavier_uniform_(self.T)\n",
        "\n",
        "        # Theta is used for weighting coefficients in the diffusion process.\n",
        "        self.theta = nn.Parameter(torch.empty(layers, num_relation, expansion_steps))\n",
        "        nn.init.xavier_uniform_(self.theta)\n",
        "\n",
        "        # Create a lower triangular mask. Only positions with lower_tri != 0 (i.e., strictly lower triangular)\n",
        "        # will be assigned a decay value computed as gamma ** -lower_tri.\n",
        "        lower_tri = torch.tril(torch.ones(time_dim, time_dim), diagonal=-1)\n",
        "        D_gamma_tensor = torch.where(lower_tri == 0, torch.tensor(0.0), gamma ** -lower_tri)\n",
        "        # Register as a buffer so it moves with the model's device and is saved/loaded with state_dict.\n",
        "        self.register_buffer('D_gamma', D_gamma_tensor)\n",
        "\n",
        "        # Initialize Multi-relational Graph Diffusion layers.\n",
        "        self.diffusion_layers = nn.ModuleList(\n",
        "            [MultiReDiffusion(diffusion[i], diffusion[i + 1], num_relation)\n",
        "             for i in range(len(diffusion) - 1)]\n",
        "        )\n",
        "\n",
        "        # Initialize Parallel Retention layers.\n",
        "        self.retention_layers = nn.ModuleList(\n",
        "            [ParallelRetention(time_dim, retention[3 * i], retention[3 * i + 1], retention[3 * i + 2])\n",
        "             for i in range(len(retention) // 3)]\n",
        "        )\n",
        "\n",
        "        # Initialize decoupled transformation layers.\n",
        "        self.ret_linear_1 = nn.ModuleList(\n",
        "            [nn.Linear(ret_linear_1[2 * i], ret_linear_1[2 * i + 1])\n",
        "             for i in range(len(ret_linear_1) // 2)]\n",
        "        )\n",
        "        self.ret_linear_2 = nn.ModuleList(\n",
        "            [nn.Linear(ret_linear_2[2 * i], ret_linear_2[2 * i + 1])\n",
        "             for i in range(len(ret_linear_2) // 2)]\n",
        "        )\n",
        "\n",
        "        # MLP layers for post-processing.\n",
        "        self.mlp = nn.ModuleList(\n",
        "            [nn.Linear(post_pro[i], post_pro[i + 1]) for i in range(len(post_pro) - 1)]\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, a):\n",
        "        \"\"\"\n",
        "        x: input tensor (e.g., node features); expected shape should be (batch_size, num_nodes, feature_dim)\n",
        "        a: adjacency (or relation) information for graph diffusion.\n",
        "        \"\"\"\n",
        "        # Use the same device as the input x.\n",
        "        device = x.device\n",
        "\n",
        "        # Ensure h is on the proper device.\n",
        "        h = x.to(device)\n",
        "\n",
        "        # Information diffusion and graph representation learning\n",
        "        for l in range(self.layers):\n",
        "            # Multi-relational Graph Diffusion layer:\n",
        "            # The diffusion layer returns updated h and an intermediate representation u.\n",
        "            h, u = self.diffusion_layers[l](self.theta[l], self.T[l], a, h)\n",
        "\n",
        "            # Ensure u is on the same device as D_gamma.\n",
        "            u = u.to(device)\n",
        "\n",
        "            # Parallel Retention layer:\n",
        "            # The retention layer expects u and the decay matrix D_gamma.\n",
        "            eta = self.retention_layers[l](u, self.D_gamma)\n",
        "\n",
        "            # Decoupled representation transform:\n",
        "            # For the first layer, combine the eta representation with a transformed version of the\n",
        "            # original input.\n",
        "            if l == 0:\n",
        "                # Reshape x to (num_nodes, -1) so that it aligns with eta.\n",
        "                x_reshaped = x.view(x.shape[1], -1)\n",
        "                h_concat = torch.cat((eta, self.ret_linear_1[l](x_reshaped)), dim=1)\n",
        "                h_prime = self.ret_linear_2[l](h_concat)\n",
        "            else:\n",
        "                h_concat = torch.cat((eta, self.ret_linear_1[l](h_prime)), dim=1)\n",
        "                h_prime = self.ret_linear_2[l](h_concat)\n",
        "\n",
        "        # Post-processing with MLP layers to generate final graph representation.\n",
        "        for mlp_layer in self.mlp:\n",
        "            h_prime = mlp_layer(h_prime)\n",
        "\n",
        "        return h_prime\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"\n",
        "        Reset learnable model parameters using appropriate initialization methods.\n",
        "        Note that D_gamma is not learnable and is registered as a buffer.\n",
        "        \"\"\"\n",
        "        # Reinitialize T and theta with Xavier uniform initialization.\n",
        "        nn.init.xavier_uniform_(self.T)\n",
        "        nn.init.xavier_uniform_(self.theta)\n",
        "\n",
        "        # Optionally, you could also reset the parameters of the submodules.\n",
        "        for module in self.modules():\n",
        "            if hasattr(module, 'reset_parameters') and module not in [self]:\n",
        "                module.reset_parameters()\n"
      ],
      "metadata": {
        "id": "uYgp--Bx6vqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Train/Validate/Test***"
      ],
      "metadata": {
        "id": "kMVEl80d7b0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv as csv\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions\n",
        "from sklearn.metrics import matthews_corrcoef, f1_score\n",
        "\n",
        "# Configure the device for running the model on GPU or CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Configure the default variables // # these can be tuned // # examples\n",
        "sedate = ['2013-01-01', '2014-12-31']  # these can be tuned\n",
        "val_sedate = ['2015-01-01', '2015-06-30'] # these can be tuned\n",
        "test_sedate = ['2015-07-01', '2017-12-31'] # these can be tuned\n",
        "market = ['NASDAQ', 'NYSE', 'SSE'] # can be changed\n",
        "dataset_type = ['Train', 'Validation', 'Test']\n",
        "com_path = ['/content/drive/MyDrive/Raw_Data/Stock_Markets/NYSE_NASDAQ/NASDAQ.csv',\n",
        "            '/content/drive/MyDrive/Raw_Data/Stock_Markets/NYSE_NASDAQ/NYSE.csv',\n",
        "            '/content/drive/MyDrive/Raw_Data/Stock_Markets/NYSE_NASDAQ/NYSE_missing.csv']\n",
        "des = '/content/drive/MyDrive/Raw_Data/Stock_Markets/NYSE_NASDAQ/raw_stock_data/stocks_indicators/data'\n",
        "directory = \"/content/drive/MyDrive/Raw_Data/Stock_Markets/NYSE_NASDAQ/raw_stock_data/stocks_indicators/data/google_finance\"\n",
        "\n",
        "NASDAQ_com_list = []\n",
        "NYSE_com_list = []\n",
        "NYSE_missing_list = []\n",
        "com_list = [NASDAQ_com_list, NYSE_com_list, NYSE_missing_list]\n",
        "for idx, path in enumerate(com_path):\n",
        "    with open(path) as f:\n",
        "        file = csv.reader(f)\n",
        "        for line in file:\n",
        "            com_list[idx].append(line[0])  # append first element of line if each line is a list\n",
        "NYSE_com_list = [com for com in NYSE_com_list if com not in NYSE_missing_list]\n",
        "\n",
        "# Generate datasets\n",
        "train_dataset = MyDataset(directory, des, market[0], NASDAQ_com_list, sedate[0], sedate[1], 19, dataset_type[0])\n",
        "validation_dataset = MyDataset(directory, des, market[0], NASDAQ_com_list, sedate[0], sedate[1], 19, dataset_type[0])\n",
        "test_dataset = MyDataset(directory, des, market[0], NASDAQ_com_list, sedate[0], sedate[1], 19, dataset_type[0])\n",
        "\n",
        "# Define model (these can be tuned)\n",
        "n = len(NASDAQ_com_list) # number of companies in NASDAQ\n",
        "\n",
        "d_layers, num_nodes, time_steps, num_relation, gamma, diffusion_steps = 6, n, 21, 5, 2.5e-4, 7\n",
        "\n",
        "diffusion_layers = [time_steps, 3 * time_steps, 4 * time_steps, 5 * time_steps, 5 * time_steps, 6 * time_steps, 5 * time_steps]\n",
        "\n",
        "retention_layers = [num_relation*3*n, num_relation*5*n, num_relation*4*n,\n",
        "                    num_relation*4*n, num_relation*5*n, num_relation*5*n,\n",
        "                    num_relation*5*n, num_relation*5*n, num_relation*5*n,\n",
        "                    num_relation*5*n, num_relation*5*n, num_relation*5*n,\n",
        "                    num_relation*6*n, num_relation*5*n, num_relation*5*n,\n",
        "                    num_relation*5*n, num_relation*5*n, num_relation*5*n]\n",
        "\n",
        "\n",
        "ret_linear_layers_1 = [time_steps * num_relation, time_steps * num_relation,\n",
        "                     time_steps * num_relation * 5, time_steps * num_relation,\n",
        "                     time_steps * num_relation * 6, time_steps * num_relation,\n",
        "                     time_steps * num_relation * 6, time_steps * num_relation,\n",
        "                     time_steps * num_relation * 6, time_steps * num_relation,\n",
        "                     time_steps * num_relation * 6, time_steps * num_relation]\n",
        "\n",
        "\n",
        "ret_linear_layers_2 = [time_steps * num_relation * 5, time_steps * num_relation * 5,\n",
        "                     time_steps * num_relation * 6, time_steps * num_relation * 6,\n",
        "                     time_steps * num_relation * 6, time_steps * num_relation * 6,\n",
        "                     time_steps * num_relation * 6, time_steps * num_relation * 6,\n",
        "                     time_steps * num_relation * 6, time_steps * num_relation * 6,\n",
        "                     time_steps * num_relation * 6, time_steps * num_relation * 6]\n",
        "\n",
        "mlp_layers = [num_relation * 5 * time_steps + time_steps * num_relation, 128, 2]\n",
        "\n",
        "# Define model\n",
        "model = MGDPR(diffusion_layers, retention_layers, ret_linear_layers_1, ret_linear_layers_2, mlp_layers, d_layers,\n",
        "              num_nodes, time_steps, num_relation, gamma, diffusion_steps)\n",
        "\n",
        "# Pass model and datasets to GPU\n",
        "model = model.to(device)\n",
        "\n",
        "# Define optimizer and objective function\n",
        "\n",
        "\n",
        "def theta_regularizer(theta):\n",
        "    row_sums = torch.sum(theta.to(device), dim=-1)\n",
        "    ones = torch.ones_like(row_sums)\n",
        "    return torch.sum(torch.abs(row_sums - ones))\n",
        "\n",
        "\n",
        "#def D_gamma_regularizer(D_gamma):\n",
        "    #upper_tri = torch.triu(D_gamma, diagonal=1)\n",
        "    #return torch.sum(torch.abs(upper_tri))\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Define training process & validation process & testing process\n",
        "epochs = 10000\n",
        "model.reset_parameters()\n",
        "\n",
        "# Training and validating\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    objective_total = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for sample in train_dataset: # Recommand to update every sample, full batch training can be time-consuming\n",
        "        X = sample['X'].to(device)  # node feature tensor\n",
        "        A = sample['A'].to(device)  # adjacency tensor\n",
        "        C = sample['Y'].long()\n",
        "        C = C.to(device)  # label vector\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(X, A)\n",
        "        objective = F.cross_entropy(out, C) #+ theta_regularizer(model.theta) regularization may resultvery slow learning process, optional usage.\n",
        "        objective.backward()\n",
        "        optimizer.step()\n",
        "        objective_total += objective.item()\n",
        "\n",
        "    # If performance progress of the model is required\n",
        "        out = model(X, A).argmax(dim=1)\n",
        "        correct += int((out == C).sum()).item()\n",
        "        total += C.shape[0]\n",
        "        if epoch % 1 == 0:\n",
        "          print(f\"Epoch {epoch}: loss={objective_total:.4f}, acc={correct / total:.4f}\")\n",
        "\n",
        "# Validation\n",
        "model.eval()\n",
        "acc = 0\n",
        "f1 = 0\n",
        "mcc = 0\n",
        "\n",
        "for idx, sample in enumerate(validation_dataset):\n",
        "    X = sample['X']  # node feature tensor\n",
        "    A = sample['A']  # adjacency tensor\n",
        "    C = sample['Y']  # label vector\n",
        "    out = model(X, A).argmax(dim=1)\n",
        "\n",
        "    acc += int((out == C).sum())\n",
        "    f1 += f1_score(C, out.cpu().numpy())\n",
        "    mcc += matthews_corrcoef(C, out.cpu().numpy())\n",
        "\n",
        "print(acc / (len(validation_dataset) * C.shape[0]))\n",
        "print(f1 / len(validation_dataset))\n",
        "print(mcc / len(validation_dataset))\n",
        "\n",
        "# Test\n",
        "acc = 0\n",
        "f1 = 0\n",
        "mcc = 0\n",
        "\n",
        "for idx, sample in enumerate(test_dataset):\n",
        "    X = sample['X']  # node feature tensor\n",
        "    A = sample['A']  # adjacency tensor\n",
        "    C = sample['Y']  # label vector\n",
        "    out = model(X, A).argmax(dim=1)\n",
        "\n",
        "    acc += int((out == C).sum())\n",
        "    f1 += f1_score(C, out.cpu().numpy())\n",
        "    mcc += matthews_corrcoef(C, out.cpu().numpy())\n",
        "\n",
        "print(acc / (len(test_dataset) * C.shape[0]))\n",
        "print(f1 / len(test_dataset))\n",
        "print(mcc / len(test_dataset))\n",
        "\n",
        "# save model to the directory\n",
        "if int(input('save model? (1/0)?')) == 1:\n",
        "    torch.save(model, dir_path() + 'your_dataset_name/model')"
      ],
      "metadata": {
        "id": "F_-OAE1Q7hxl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}